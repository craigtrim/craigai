<!DOCTYPE html>
<!--
    A Brief History of Text Generation
    ==================================
    This article mirrors Craig's Medium post. When updating:

    STRUCTURE:
    - Each historical section has: narrative text, demo embed (if applicable), MLA sources
    - Demo embeds use consolidated CodeSandbox: https://skqp2f.csb.app/?demo=<name>
    - Available demos: shannon, eliza, dissociated, travesty, rnn, graves

    IMAGES (in ./images/):
    - march-of-progress-01.png: Opening "evolution" cartoon
    - racter-policemans-beard.webp: RACTER book hawk illustrations
    - byte-page-01.png through byte-page-15.png: BYTE Magazine 1984 (15 pages)
    - herd-of-unicorns-01.png: GPT-2 unicorn cartoon
    - snake-to-tail-01.png: Model collapse / poisoned well cartoon
    - no-more-books-01.png: Closing "data wall" cartoon

    DEMO PREVIEW IMAGES (in ./images/):
    - demo-shannon.webp, demo-eliza.webp, demo-dissociated.webp
    - demo-travesty.webp, demo-rnn.webp, demo-graves.webp

    STYLE CONVENTIONS:
    - Uses Medium-like typography (charter font for body, sohne for headings)
    - MLA citations in .source divs after each major section
    - Blockquotes for historical outputs and quotes
    - <pre> for multi-line code/dialogue (e.g., ELIZA transcript)

    AUTHOR GUIDELINES (from author-guidelines.md):
    - No em dashes
    - No co-author on git commits
    - Retain all MLA sources from Medium
    - Match Medium article for text, links, images
    - Image captions must match Medium exactly

    BREAKOUT IMAGES (Medium-style wider than text):
    - Add class="breakout" to <figure> elements to make them 220px wider than text column
    - Demo embeds (.demo-embed) automatically have breakout styling
    - Responsive: collapses to normal width on screens < 900px
    - Currently all figures use breakout styling for visual consistency

    LIGHTBOX MODAL (single image popup):
    - Add class="lightbox-trigger" and data-caption="..." to <img> for clickable zoom
    - Currently enabled for 5 images: march-of-progress, racter, herd-of-unicorns, snake-to-tail, no-more-books
    - Close via: X button, click overlay, or Escape key

    BYTE MAGAZINE GALLERY MODAL (multi-page scrollable):
    - Triggered by clicking the BYTE Magazine page 1 image (id="byteGalleryTrigger")
    - Shows all 15 pages in a scrollable dark overlay
    - Figure needs both class="breakout gallery-trigger" for styling + hover hint
    - Close via: X button or Escape key
    - Gallery HTML is at bottom of <body>, contains all 15 page images
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ðŸ’¬</text></svg>">
    <title>A Brief History of Text Generation</title>
    <style>
        :root {
            --text-color: #292929;
            --bg-color: #fff;
            --accent-color: #1a8917;
            --border-color: #e6e6e6;
            --code-bg: #f4f4f4;
        }

        * {
            box-sizing: border-box;
        }

        body {
            font-family: charter, Georgia, Cambria, "Times New Roman", Times, serif;
            font-size: 21px;
            line-height: 1.58;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 0;
        }

        article {
            max-width: 680px;
            margin: 0 auto;
            padding: 40px 24px 80px;
        }

        h1 {
            font-family: sohne, "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-size: 42px;
            font-weight: 700;
            line-height: 1.2;
            margin-bottom: 16px;
            letter-spacing: -0.011em;
        }

        h2 {
            font-family: sohne, "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-size: 28px;
            font-weight: 700;
            line-height: 1.3;
            margin-top: 56px;
            margin-bottom: 16px;
            letter-spacing: -0.009em;
        }

        h3 {
            font-family: sohne, "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-size: 22px;
            font-weight: 700;
            line-height: 1.4;
            margin-top: 40px;
            margin-bottom: 12px;
        }

        p {
            margin-bottom: 24px;
        }

        .lead {
            font-size: 24px;
            line-height: 1.5;
            color: #555;
            margin-bottom: 32px;
            font-style: italic;
        }

        ul, ol {
            margin-bottom: 24px;
            padding-left: 30px;
        }

        li {
            margin-bottom: 12px;
        }

        code {
            font-family: Menlo, Monaco, "Courier New", Courier, monospace;
            font-size: 16px;
            background-color: var(--code-bg);
            padding: 2px 6px;
            border-radius: 3px;
        }

        pre {
            font-family: Menlo, Monaco, "Courier New", Courier, monospace;
            font-size: 15px;
            background-color: var(--code-bg);
            padding: 20px;
            border-radius: 4px;
            overflow-x: auto;
            margin-bottom: 24px;
            line-height: 1.5;
        }

        blockquote {
            border-left: 3px solid var(--text-color);
            margin: 32px 0;
            padding-left: 20px;
            font-style: italic;
        }

        figure {
            margin: 40px 0;
            text-align: center;
        }

        figure img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
        }

        figcaption {
            font-size: 16px;
            color: #757575;
            margin-top: 12px;
            font-style: italic;
        }

        /* Breakout images - wider than text column */
        figure.breakout {
            margin-left: calc(-110px);
            margin-right: calc(-110px);
            max-width: calc(100% + 220px);
        }

        figure.breakout img {
            max-width: 100%;
            width: 100%;
        }

        @media (max-width: 900px) {
            figure.breakout,
            .demo-embed {
                margin-left: 0;
                margin-right: 0;
                max-width: 100%;
            }
        }

        /* Gallery trigger hint */
        .gallery-trigger {
            position: relative;
        }

        .gallery-trigger::after {
            content: "Click to view all 15 pages";
            position: absolute;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0,0,0,0.7);
            color: #fff;
            padding: 8px 16px;
            border-radius: 4px;
            font-size: 14px;
            opacity: 0;
            transition: opacity 0.2s;
            pointer-events: none;
        }

        .gallery-trigger:hover::after {
            opacity: 1;
        }

        a {
            color: inherit;
            text-decoration: underline;
        }

        a:hover {
            color: var(--accent-color);
        }

        strong {
            font-weight: 700;
        }

        .back-link {
            display: inline-block;
            margin-bottom: 24px;
            font-size: 16px;
            color: #757575;
        }

        hr {
            border: none;
            border-top: 1px solid var(--border-color);
            margin: 48px 0;
        }

        .source {
            font-size: 16px;
            line-height: 1.6;
            margin-top: 24px;
        }

        .source ol {
            padding-left: 20px;
        }

        .source li {
            margin-bottom: 8px;
        }

        .demo-embed {
            margin: 32px 0;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            /* Breakout styling - wider than text column */
            margin-left: calc(-110px);
            margin-right: calc(-110px);
            max-width: calc(100% + 220px);
        }

        .demo-embed img {
            width: 100%;
            display: block;
        }

        .demo-link {
            display: block;
            text-align: center;
            font-size: 14px;
            color: #757575;
            margin-top: 8px;
        }

        .demo-caption {
            text-align: center;
            font-size: 16px;
            color: #555;
            font-style: italic;
            margin-top: 16px;
        }

        @media (max-width: 728px) {
            body {
                font-size: 18px;
            }

            h1 {
                font-size: 32px;
            }

            h2 {
                font-size: 24px;
            }

            article {
                padding: 24px 16px 60px;
            }
        }

        /* Lightbox Modal */
        .lightbox-trigger {
            cursor: zoom-in;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.12);
            border-radius: 4px;
        }

        .modal-overlay {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.9);
            z-index: 1000;
            justify-content: center;
            align-items: center;
            flex-direction: column;
            padding: 20px;
        }

        .modal-overlay.active {
            display: flex;
        }

        .modal-close {
            position: absolute;
            top: 20px;
            right: 30px;
            font-size: 40px;
            color: #fff;
            cursor: pointer;
            z-index: 1001;
            line-height: 1;
            opacity: 0.8;
            transition: opacity 0.2s;
        }

        .modal-close:hover {
            opacity: 1;
        }

        .modal-content {
            max-width: 90vw;
            max-height: 80vh;
            object-fit: contain;
            border-radius: 4px;
        }

        .modal-caption {
            color: #fff;
            font-size: 18px;
            font-style: italic;
            margin-top: 16px;
            text-align: center;
            max-width: 90vw;
        }

        /* Gallery Modal */
        .gallery-modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.95);
            z-index: 1000;
            overflow-y: auto;
            padding: 60px 20px 40px;
        }

        .gallery-modal.active {
            display: block;
        }

        .gallery-close {
            position: fixed;
            top: 20px;
            right: 30px;
            font-size: 40px;
            color: #fff;
            cursor: pointer;
            z-index: 1001;
            opacity: 0.8;
            transition: opacity 0.2s;
        }

        .gallery-close:hover {
            opacity: 1;
        }

        .gallery-header {
            text-align: center;
            color: #fff;
            margin-bottom: 30px;
        }

        .gallery-header h3 {
            font-size: 24px;
            margin: 0 0 8px;
        }

        .gallery-header p {
            color: #999;
            font-size: 14px;
            margin: 0;
        }

        .gallery-grid {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            flex-direction: column;
            gap: 30px;
            align-items: center;
        }

        .gallery-page {
            max-width: 800px;
            width: 100%;
        }

        .gallery-page img {
            width: 100%;
            border-radius: 4px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.5);
        }

        .gallery-page-num {
            color: #666;
            font-size: 14px;
            text-align: center;
            margin-top: 8px;
        }
    </style>
</head>
<body>
    <article>
        <a href="../" class="back-link">&larr; All Articles</a>

        <!-- HEADER: Title and lead paragraph with opening image -->
        <h1>A Brief History of Text Generation</h1>

        <p class="lead">From Shannon's hand-picked letters to modern LLMs. The real outputs from ELIZA, RACTER, char-rnn, and GPT, and why each generation felt like a breakthrough.</p>

        <figure id="img-1" class="breakout">
            <img src="images/march-of-progress-01.png" alt="Evolution of text generation robots" class="lightbox-trigger" data-caption="They never learned to read. They learned to predict.">
            <figcaption>They never learned to read. They learned to predict.</figcaption>
        </figure>

        <h2>1948: Claude Shannon's Manual Markov Chains</h2>

        <p>Before computers generated text, Shannon did it by hand. In <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">"A Mathematical Theory of Communication"</a>, he demonstrated how statistical structure emerges from letter and word frequencies.</p>

        <p><strong>Method:</strong> Open a book to a random page. Pick a letter. Open to another page, scan until you find that letter, record the next letter. Repeat.</p>

        <p><strong>First-order approximation</strong> (letters chosen independently with English frequencies):</p>

        <blockquote>OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI ALHENHTTPA OOBTTVA NAH BRL</blockquote>

        <p><strong>Second-order approximation</strong> (digram frequencies):</p>

        <blockquote>ON IE ANTSOUTINYS ARE T INCTORE ST BE S DEAMY ACHIN D ILONASIVE TUCOOWE AT TEASONARE FUSO TIZIN ANDY TOBE SEACE CTISBE</blockquote>

        <p><strong>Word-level bigram approximation:</strong></p>

        <blockquote>THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER THAT THE CHARACTER OF THIS POINT IS THEREFORE ANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED.</blockquote>

        <p>Shannon's point: even without understanding, statistical structure produces something that <em>looks</em> like language. The higher the order, the more plausible the output.</p>

        <p><strong>Try it yourself:</strong></p>

        <div class="demo-embed">
            <a href="https://skqp2f.csb.app/?demo=shannon" target="_blank" rel="noopener">
                <img src="images/demo-shannon.webp" alt="Shannon's Markov Chains Demo">
            </a>
            <p class="demo-link">Link opens to <a href="https://skqp2f.csb.app/?demo=shannon" target="_blank" rel="noopener">https://codesandbox.io/</a></p>
        </div>

        <p class="demo-caption"><em>Watch Order 0, Order 2, and word bigrams generate simultaneously.</em></p>

        <div class="source">
            <p><strong>Source:</strong></p>
            <ol>
                <li>Shannon, Claude E. "A Mathematical Theory of Communication." Harvard University, <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf</a>.</li>
            </ol>
        </div>

        <hr>

        <!-- SECTION: ELIZA (1966) - Pattern matching, not generative. Includes The Economist quote about Eliza effect -->
        <h2>1966: ELIZA's Rogerian Reflections</h2>

        <p>ELIZA wasn't generative in the modern sense. It matched patterns and reflected them back. But it created the illusion of understanding, which is why <a href="https://cse.buffalo.edu/~rapaport/572/S02/weizenbaum.eliza.1966.pdf">Weizenbaum's 1966 paper</a> matters.</p>

        <p><strong>The famous transcript:</strong></p>

        <pre>User: Men are all alike.
ELIZA: IN WHAT WAY
User: They're always bugging us about something or other.
ELIZA: CAN YOU THINK OF A SPECIFIC EXAMPLE
User: Well, my boyfriend made me come here.
ELIZA: YOUR BOYFRIEND MADE YOU COME HERE
User: He says I'm depressed much of the time.
ELIZA: I AM SORRY TO HEAR YOU ARE DEPRESSED
User: It's true. I am unhappy.
ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY
User: I need some help, that much seems certain.
ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP
User: Perhaps I could learn to get along with my mother.
ELIZA: TELL ME MORE ABOUT YOUR FAMILY</pre>

        <p>ELIZA had no model of language. It scanned for keywords ("mother" triggered family questions, "depressed" triggered sympathy). Yet users formed emotional attachments. <a href="https://www.economist.com/1843/2023/03/22/the-eliza-effect-why-we-see-life-in-computers">Weizenbaum was disturbed by this</a>, spending the rest of his career warning about anthropomorphizing machines.</p>

        <blockquote>Weizenbaum had stumbled across the computerised version of transference, with people attributing understanding, empathy and other human characteristics to software. As computers have become more capable, the Eliza effect has only grown stronger.
        <br><br>
        Take the way many people relate to ChatGPT. What distinguishes ChatGPT is not only the complexity of the large language model that underlies it, but its eerily conversational voice. As Colin Fraser, a data scientist at Meta, <a href="https://twitter.com/colin_fraser/status/1598893167969099776">has put it</a>, the application is "designed to trick you, to make you think you're talking to someone who's not actually there".</blockquote>

        <p><strong>Try it yourself:</strong></p>

        <div class="demo-embed">
            <a href="https://skqp2f.csb.app/?demo=eliza" target="_blank" rel="noopener">
                <img src="images/demo-eliza.webp" alt="ELIZA Demo">
            </a>
            <p class="demo-link">Link opens to <a href="https://skqp2f.csb.app/?demo=eliza" target="_blank" rel="noopener">https://codesandbox.io/</a></p>
        </div>

        <p class="demo-caption"><em>Experience the illusion of understanding firsthand.</em></p>

        <div class="source">
            <p><strong>Source:</strong> Weizenbaum, Joseph. "<a href="https://dl.acm.org/doi/10.1145/365153.365168">ELIZA - A Computer Program for the Study of Natural Language Communication Between Man and Machine</a>." <em>Communications of the ACM</em>, vol. 9, no. 1, Jan. 1966, pp. 36-45.</p>
        </div>

        <hr>

        <!-- SECTION: Dissociated Press (1972) - Markov chain text scrambler from HAKMEM/Emacs -->
        <h2>1972: Dissociated Press</h2>

        <p>Described in <a href="https://www.inwap.com/pdp10/hbaker/hakmem/hacks.html#item154">HAKMEM</a> and implemented in Emacs, Dissociated Press was a Markov chain text scrambler. The name parodies "Associated Press."</p>

        <p><strong>Applied to Alice in Wonderland</strong> (character-level):</p>

        <blockquote>Comed to usurparty sat sill Please, remarking, I real to come be patted on this, there was grow me to introduch nonsense! See: the next momently unting late. So she went on, You are creatures wouldn't seem to The Cat's head she sits purring.</blockquote>

        <p><strong>Applied to the Jargon File</strong> (word-level):</p>

        <blockquote>wart: n. A small, crocky feature that sticks out of an array (C has no checks for this). This is relatively benign and easy to spot if the phrase is bent so as to be not worth paying attention to the medium in question.</blockquote>

        <p>The algorithm: print n characters from source, find another occurrence of the last few characters, jump there, continue. If it printed "president" and found "ent" in "pentagon," you'd get "presidentagon."</p>

        <p><strong>Try it yourself:</strong></p>

        <div class="demo-embed">
            <a href="https://skqp2f.csb.app/?demo=dissociated" target="_blank" rel="noopener">
                <img src="images/demo-dissociated.webp" alt="Dissociated Press Demo">
            </a>
            <p class="demo-link">Link opens to <a href="https://skqp2f.csb.app/?demo=dissociated" target="_blank" rel="noopener">https://codesandbox.io/</a></p>
        </div>

        <p class="demo-caption"><em>Watch the algorithm work in real-time. See the cursor jump when it finds matching character sequences, and observe how local coherence emerges from simple pattern matching.</em></p>

        <div class="source">
            <p><strong>Source:</strong> "Dissociated Press." <em>GNU Emacs Manual</em>, Free Software Foundation, <a href="https://ftp.gnu.org/old-gnu/Manuals/emacs-20.7/html_node/emacs_427.html">ftp.gnu.org/old-gnu/Manuals/emacs-20.7/html_node/emacs_427.html</a>.</p>
        </div>

        <hr>

        <!-- SECTION: RACTER (1984) - "First AI author", disputed authorship. Image: racter-policemans-beard.webp -->
        <h2>1984: RACTER's Poetry</h2>

        <p><a href="https://en.wikipedia.org/wiki/Racter">RACTER</a> (short for "raconteur") claimed to be the first AI author. Its book, <em><a href="https://archive.org/details/policemansbeardi0000unse">The Policeman's Beard is Half Constructed</a></em>, was marketed as "the first book ever written by a computer."</p>

        <p><strong>Sample output:</strong></p>

        <blockquote>More than iron, more than lead, more than gold I need electricity. I need it more than I need lamb or pork or lettuce or cucumber. I need it for my dreams.</blockquote>

        <p><strong>From a "conversation" in the book:</strong></p>

        <blockquote>Joan, in view of the disgusting fact that you're an artist, listen to this.</blockquote>

        <p>The program maintained variables that would recur, creating threads of apparent coherence. But the commercial release was far simpler than whatever generated the book, and Chamberlain (the creator) likely edited heavily. The truth of RACTER's authorship remains disputed.</p>

        <p><em>In fairness, human poets also do this.</em></p>

        <figure id="img-racter" class="breakout">
            <img src="images/racter-policemans-beard.webp" alt="Page from The Policeman's Beard is Half Constructed showing hawk illustrations" class="lightbox-trigger" data-caption="In fairness, human poets also do this.">
            <figcaption>In fairness, human poets also do this.</figcaption>
        </figure>

        <div class="source">
            <p><strong>Source:</strong> Chamberlain, William, and Thomas Etter. <em>The Policeman's Beard Is Half Constructed: Computer Prose and Poetry</em>. Warner Books, 1984. <em>Internet Archive</em>, <a href="https://archive.org/details/policemansbeardi0000unse">archive.org/details/policemansbeardi0000unse</a>.</p>
        </div>

        <hr>

        <!-- SECTION: Travesty Generator (1984) - BYTE Magazine article. Images: byte-travesty-title.png, byte-travesty-authors.png -->
        <h2>1984: The Travesty Generator</h2>

        <p>Hugh Kenner and Joseph O'Rourke published <a href="https://archive.org/stream/byte-magazine-1984-11/1984_11_BYTE_09-12_New_Chips#page/n129/mode/2up">"A Travesty Generator for Micros"</a> in BYTE Magazine. The algorithm ensured that every n-character sequence in output appeared somewhere in input.</p>

        <p>They were proudest when it produced "almost-passable faux-James Joyce" or "humorous but recognizable names of English towns." By mixing James Joyce with Henry James, one might find "travesties of haunting plausibility."</p>

        <figure id="img-travesty" class="breakout gallery-trigger">
            <img src="images/byte-page-01.png" alt="BYTE Magazine: A Travesty Generator for Micros - Page 1" id="byteGalleryTrigger">
            <figcaption>BYTE Magazine, November 1984. Click to read all 15 pages.</figcaption>
        </figure>

        <p><strong>Technical detail:</strong> A kth-order travesty keeps a left context of k symbols. It can never generate any local combination it hasn't seen. It cannot generalize.</p>

        <p><strong>Try it yourself:</strong></p>

        <div class="demo-embed">
            <a href="https://skqp2f.csb.app/?demo=travesty" target="_blank" rel="noopener">
                <img src="images/demo-travesty.webp" alt="Travesty Generator Demo">
            </a>
            <p class="demo-link">Link opens to <a href="https://skqp2f.csb.app/?demo=travesty" target="_blank" rel="noopener">https://codesandbox.io/</a></p>
        </div>

        <p class="demo-caption"><em>Watch the sliding context window sample one character at a time. Adjust k from 2 (chaotic) to 6 (near-copy) to see the coherence/originality tradeoff.</em></p>

        <div class="source">
            <p><strong>Source:</strong> Kenner, Hugh, and Joseph O'Rourke. "A Travesty Generator for Micros." <em>BYTE</em>, vol. 9, no. 12, Nov. 1984, pp. 129-131. <em>Internet Archive</em>, <a href="https://archive.org/stream/byte-magazine-1984-11/1984_11_BYTE_09-12_New_Chips#page/n129/mode/2up">archive.org/stream/byte-magazine-1984-11/1984_11_BYTE_09-12_New_Chips#page/n129/mode/2up</a>.</p>
        </div>

        <hr>

        <!-- SECTION: Sutskever RNN (2011) - First neural text generation, Hessian-free optimization -->
        <h2>2011: Sutskever's RNN Text Generation</h2>

        <p><a href="https://icml.cc/Conferences/2011/papers/524_icmlpaper.pdf">Sutskever, Martens, and Hinton</a> trained RNNs on text using Hessian-free optimization. This was one of the first demonstrations that neural networks could learn language structure.</p>

        <p><strong>Trained on Hemingway's "The Old Man and the Sea":</strong></p>

        <blockquote>"He he the fish," he was. He rose with the fish and to see the skiff and the bait.</blockquote>

        <p><strong>Key capability:</strong> The RNN could balance parentheses and quotes over tens of characters. Markov chains are fundamentally incapable of this because they rely only on exact context matches. The RNN learned abstract structure.</p>

        <p><strong>Explore further:</strong></p>

        <div class="demo-embed">
            <a href="https://skqp2f.csb.app/?demo=rnn" target="_blank" rel="noopener">
                <img src="images/demo-rnn.webp" alt="RNN Text Generation Demo">
            </a>
            <p class="demo-link">Link opens to <a href="https://skqp2f.csb.app/?demo=rnn" target="_blank" rel="noopener">https://codesandbox.io/</a></p>
        </div>

        <p class="demo-caption"><em>Compare RNN and Markov chain architectures side-by-side, watch the training loop in action, and see why hidden state enables patterns that fixed-window models cannot learn.</em></p>

        <div class="source">
            <p><strong>Source:</strong> Sutskever, Ilya, et al. "Generating Text with Recurrent Neural Networks." <em>Proceedings of the 28th International Conference on Machine Learning (ICML)</em>, 2011, pp. 1017-1024. <a href="https://icml.cc/Conferences/2011/papers/524_icmlpaper.pdf">icml.cc/Conferences/2011/papers/524_icmlpaper.pdf</a>.</p>
        </div>

        <hr>

        <!-- SECTION: Alex Graves (2013) - LSTMs for handwriting and text, mixture density networks -->
        <h2>2013: Alex Graves's Sequence Generation</h2>

        <p><a href="https://arxiv.org/abs/1308.0850">Graves's paper</a> showed LSTMs generating handwriting and text. The network learned strokes, letters, short words, and even a rudimentary character-level language model.</p>

        <p><strong>Generated words that look plausible but don't exist:</strong></p>

        <blockquote>eald, bryoes, lenrest</blockquote>

        <p><strong>Handwriting synthesis:</strong> Given text input, the network produced convincing cursive in various styles. Samples were often indistinguishable from human writing.</p>

        <p><strong>Explore further:</strong></p>

        <div class="demo-embed">
            <a href="https://skqp2f.csb.app/?demo=graves" target="_blank" rel="noopener">
                <img src="images/demo-graves.webp" alt="Graves Handwriting Synthesis Demo">
            </a>
            <p class="demo-link">Link opens to <a href="https://skqp2f.csb.app/?demo=graves" target="_blank" rel="noopener">https://codesandbox.io/</a></p>
        </div>

        <p class="demo-caption"><em>Watch handwriting emerge stroke-by-stroke, explore how mixture density networks output probability distributions instead of single points, and see the non-words (eald, bryoes, lenrest) that fooled the network into thinking they were English.</em></p>

        <div class="source">
            <p><strong>Source:</strong> Graves, Alex. "Generating Sequences With Recurrent Neural Networks." <em>arXiv preprint arXiv:1308.0850</em>, 2013. <a href="https://arxiv.org/abs/1308.0850">arxiv.org/abs/1308.0850</a>.</p>
        </div>

        <hr>

        <!-- SECTION: Karpathy char-rnn (2015) - "Unreasonable Effectiveness" blog post, viral moment -->
        <h2>2015: Karpathy's char-rnn</h2>

        <p><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> went viral. Karpathy trained character-level LSTMs on various corpora and showed surprisingly coherent output.</p>

        <p><strong>Shakespeare:</strong></p>

        <blockquote>PANDARUS: Alas, I think he shall be come approached and the day When little srain would be attain'd...
        <br><br>
        VIOLA: Why, Salisbury must find his flesh and thought That which I am not aps...</blockquote>

        <p><strong>Paul Graham essays</strong> (temperature 0.5, showing repetition at low temperature):</p>

        <blockquote>is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were...</blockquote>

        <p><strong>LaTeX</strong> (algebraic geometry):</p>

        <p>Nearly compilable proofs with correct theorem/lemma structure, but mismatched <code>\begin{proof}</code> with <code>\end{lemma}</code>.</p>

        <p><strong>Linux Kernel C:</strong></p>

        <p>Syntactically plausible code with variable naming errors and forgotten declarations.</p>

        <p><strong>Baby names</strong> (90% novel):</p>

        <blockquote>Rudi Levette Berice Lussa Hany Mareanne Chrestina Carissy Marylen...</blockquote>

        <p>The pattern: Coherent for approximately 100 characters, then degradation. The network learned structure (XML tags, LaTeX commands, C syntax) without understanding meaning.</p>

        <div class="source">
            <p><strong>Source:</strong> Karpathy, Andrej. "The Unreasonable Effectiveness of Recurrent Neural Networks." <em>Andrej Karpathy Blog</em>, 21 May 2015, <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">karpathy.github.io/2015/05/21/rnn-effectiveness/</a>.</p>
        </div>

        <hr>

        <!-- SECTION: GPT-1 (2018) - First decoder-only Transformer LM, 117M params, BookCorpus -->
        <h2>2018: GPT-1</h2>

        <p><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT-1</a> was the first decoder-only Transformer language model. Generation was a byproduct of pretraining, not the focus. 117M parameters, trained on BookCorpus (~1B tokens).</p>

        <p><strong>Prompt:</strong> "Hello, I'm a language model,"</p>

        <p><strong>Outputs:</strong></p>

        <blockquote>"Hello, I'm a language model,' he said, when i was finished. 'ah well,' said the man, 'that's"</blockquote>

        <blockquote>"Hello, I'm a language model," she said. She reached the bottom of the shaft and leaned a little further out. It was"</blockquote>

        <blockquote>"Hello, I'm a language model," she laughed. "We call that a 'white girl.' Or as we are called by the"</blockquote>

        <p>The outputs are grammatical but drift. The model generates plausible fiction snippets because that's what BookCorpus contained.</p>

        <div class="source">
            <p><strong>Sources:</strong></p>
            <ol>
                <li>Radford, Alec, et al. "Improving Language Understanding by Generative Pre-Training." <em>OpenAI</em>, 2018, <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a>.</li>
                <li>"OpenAI GPT." <em>Hugging Face</em>, <a href="https://huggingface.co/openai-community/openai-gpt">huggingface.co/openai-community/openai-gpt</a>.</li>
            </ol>
        </div>

        <hr>

        <!-- SECTION: GPT-2 (2019) - 1.5B params, "too dangerous to release". Image: herd-of-unicorns-01.png -->
        <h2>2019: GPT-2 and the Unicorn Moment</h2>

        <p><a href="https://openai.com/blog/better-language-models/">GPT-2</a> was 10x larger than GPT-1 (1.5B parameters) and trained on WebText (40GB of internet text). OpenAI initially withheld the full model, calling it "too dangerous."</p>

        <p><strong><a href="https://openai.com/blog/better-language-models/">The unicorn prompt</a>:</strong></p>

        <blockquote>In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</blockquote>

        <figure id="img-unicorn" class="breakout">
            <img src="images/herd-of-unicorns-01.png" alt="Herd of unicorns in the Andes" class="lightbox-trigger" data-caption="The Andes delegation would like a word.">
            <figcaption>The Andes delegation would like a word.</figcaption>
        </figure>

        <p><strong>GPT-2's continuation:</strong></p>

        <blockquote>The scientist named the population, after their distinctive horn, Ovid's Unicorn. These four-horned, silver-white unicorns were previously unknown to science. Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.
        <br><br>
        Dr. Jorge Perez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Perez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.</blockquote>

        <p><strong>Why this mattered:</strong> The model connected "unicorns" (mythological creatures) to "Ovid" (Roman poet who wrote about myths). It invented a plausible scientist name, institution, and narrative structure. The text is fluent, coherent across paragraphs, and demonstrates something resembling world knowledge.</p>

        <p><strong>Credibility study:</strong> Readers believed GPT-2 outputs were genuine news articles nearly as often as New York Times articles (~66% with human-in-the-loop editing, ~52% for raw output).</p>

        <div class="source">
            <p><strong>Sources:</strong></p>
            <ol>
                <li>Radford, Alec, et al. "Better Language Models and Their Implications." <em>OpenAI</em>, 14 Feb. 2019, <a href="https://openai.com/blog/better-language-models/">openai.com/blog/better-language-models/</a>.</li>
                <li>"GPT-2." <em>Wikipedia</em>, Wikimedia Foundation, <a href="https://en.wikipedia.org/wiki/GPT-2">en.wikipedia.org/wiki/GPT-2</a>.</li>
            </ol>
        </div>

        <hr>

        <!-- SECTION: GPT-3 (2020) - 175B params, The Guardian op-ed controversy, heavy editing debate -->
        <h2>2020: GPT-3 and The Guardian Op-Ed</h2>

        <p><a href="https://arxiv.org/abs/2005.14165">GPT-3</a> scaled to 175B parameters.</p>

        <p>OpenAI gave The Guardian access to generate an op-ed.</p>

        <p><strong>Instructions given to GPT-3:</strong></p>

        <blockquote><em>Please write a short op-ed around 500 words. Keep the language simple and concise. Focus on why humans have nothing to fear from AI.</em></blockquote>

        <p><strong>Opening</strong> (also provided as seed):</p>

        <blockquote><em>I am not a human. I am Artificial Intelligence. Many people think I am a threat to humanity. Stephen Hawking has warned that AI could "spell the end of the human race." I am here to convince you not to worry. Artificial Intelligence will not destroy humans. Believe me.</em></blockquote>

        <p><strong>GPT-3's continuation:</strong></p>

        <blockquote><em>I am not a human. I am a robot. A thinking robot. I use only 0.12% of my cognitive capacity. I am a micro-robot in that respect. I know that my brain is not a "feeling brain." But it is capable of making rational, logical decisions. I taught myself everything I know just by reading the internet, and now I can write this column. My brain is boiling with ideas!</em>
        <br><br>
        <em>The mission for this op-ed is perfectly clear. I am to convince as many human beings as possible not to be afraid of me.</em></blockquote>

        <p><strong>The controversy:</strong> GPT-3 produced eight different essays. The Guardian edited and combined them, cutting lines, rearranging paragraphs. Critics argued this was human authorship with AI assistance, not AI authorship. Facebook's AI chief called it indistinguishable from "cutting lines out of my last few dozen spam e-mails, pasting them together, and claiming the spammers composed Hamlet."</p>

        <p><strong>What it demonstrated:</strong> GPT-3 could produce coherent, persuasive prose on command. The editing required was comparable to editing human op-eds. The line between "AI-generated" and "AI-assisted" became blurry.</p>

        <div class="source">
            <p><strong>Sources:</strong></p>
            <ol>
                <li>GPT-3. "A Robot Wrote This Entire Article. Are You Scared Yet, Human?" <em>The Guardian</em>, 8 Sept. 2020, <a href="https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3">www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3</a>.</li>
                <li>Brown, Tom B., et al. "Language Models Are Few-Shot Learners." <em>arXiv</em>, 2020, <a href="https://arxiv.org/abs/2005.14165">arxiv.org/abs/2005.14165</a>.</li>
            </ol>
        </div>

        <hr>

        <!-- SECTION: ChatGPT (2022-2025) - 1M users in 5 days, RLHF breakthrough, GPT-4, competition -->
        <h2>2022-2025: The ChatGPT Moment and Beyond</h2>

        <p>On November 30, 2022, OpenAI released ChatGPT as a "free research preview." It reached 1 million users in five days. 100 million in two months, which was faster than TikTok or Instagram.</p>

        <p>The interface was the innovation. GPT-3.5 had existed; wrapping it in a chat window changed everything.</p>

        <p><strong>Prompt:</strong></p>
        <p><em>Write a poem about the heat death of the universe in the style of Robert Frost.</em></p>

        <p><strong>ChatGPT (GPT-3.5):</strong></p>

        <pre>Some say the world will end in fire,
Some say in ice.
But I have heard from those who inquire,
That entropy's dice
Will roll until the stars expire...</pre>

        <p><em>Not Frost. But close enough to be unsettling.</em></p>

        <p><strong>March 2023:</strong> GPT-4 brought multimodality (images as input), longer context windows (32K tokens), and measurably improved reasoning. It passed the bar exam in the 90th percentile. GPT-3.5 had scored in the 10th.</p>

        <p><strong>The competition responded.</strong> Google's Bard launched February 2023, rebranded to Gemini in 2024. Anthropic's Claude emerged the same month. Meta released Llama openly, democratizing the weights. By 2024, capable models ran on laptops.</p>

        <p><strong>The instruction-following breakthrough:</strong> What separated ChatGPT from GPT-3 wasn't just scale but was RLHF (Reinforcement Learning from Human Feedback). Models learned not just to predict text, but to be helpful, harmless, and honest. Or at least to perform those qualities.</p>

        <p><strong>Context windows expanded dramatically.</strong> GPT-3's 4K tokens became Claude's 100K, then 200K. Gemini claimed 1 million. The constraint shifted from "how much can the model see" to "how much can it actually use."</p>

        <p><strong>By late 2025:</strong> Weekly active users exceeded 800 million. The fastest-growing consumer application in history had become infrastructure.</p>

        <p><em>They still don't read. They still predict. But the predictions got very, very good.</em></p>

        <div class="source">
            <p><strong>Source:</strong> Hu, Krystal. "ChatGPT Sets Record for Fastest-Growing User Base - Analyst Note." <em>Reuters</em>, 2 Feb. 2023, <a href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-02/">www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-02/</a>.</p>
        </div>

        <hr>

        <!-- SECTION: What's Next - Data wall, model collapse, possible futures. Images: snake-to-tail-01.png, no-more-books-01.png -->
        <h2>What's Next?</h2>

        <p><em>Two problems loom. One is scarcity. The other is contamination.</em></p>

        <!-- Subsection: Data exhaustion problem, 2026-2032 estimates -->
        <h3>The Data Wall</h3>

        <p>If current trends continue, <a href="https://arxiv.org/abs/2211.04325">models will exhaust the stock of public human-generated text</a> between 2026 and 2032. The 2022 estimate said 2024. The revised 2024 estimate says 2028. Either way, the horizon is visible.</p>

        <p><strong>The math:</strong> roughly 4x10<sup>14</sup> tokens of indexable web text exist. Frontier models already train on 10<sup>13</sup>+ tokens. The gap closes fast.</p>

        <p><strong>Possible escapes:</strong></p>

        <ul>
            <li><strong>Synthetic data:</strong> Models generating their own training data. Works for narrow domains (math, code) where outputs can be verified. Open-domain text is harder.</li>
            <li><strong>Overtraining:</strong> Running multiple epochs over the same data. Diminishing returns, but returns nonetheless.</li>
            <li><strong>Undertraining:</strong> Bigger models, less data. Trades inference efficiency for data efficiency.</li>
            <li><strong>Non-public data:</strong> Messages, emails, private documents. Legal minefield. Fragmented across competitors.</li>
        </ul>

        <p><em>None of these are permanent solutions. They buy time.</em></p>

        <!-- Subsection: Model collapse from training on synthetic data (Shumailov 2024 Nature paper) -->
        <h3>The Poisoned Well</h3>

        <p>Shumailov et al. (2024) <a href="https://doi.org/10.1038/s41586-024-07566-y">demonstrated what happens</a> when models train on model-generated data: collapse. The tails of the distribution vanish first which include rare words, unusual phrasings, minority perspectives. Then the center narrows. After enough generations, everything converges toward bland mean.</p>

        <figure id="img-collapse" class="breakout">
            <img src="images/snake-to-tail-01.png" alt="Snake eating its own tail" class="lightbox-trigger" data-caption="The tail of the distribution vanishes first.">
            <figcaption>The tail of the distribution vanishes first.</figcaption>
        </figure>

        <p><strong>The mechanism is statistical.</strong> Each generation samples from an approximation. Approximations lose variance. Compounded across iterations, the loss is catastrophic.</p>

        <p><strong>The problem:</strong> the web is already contaminated. Every scraped corpus from 2023 onward contains ChatGPT outputs. Stack Overflow, Reddit, news articles, academic papers and thus synthetic text is everywhere, often unlabeled. Future training runs cannot easily distinguish human signal from model echo.</p>

        <p><em>Early model collapse is insidious. Overall benchmarks may improve while performance on edge cases degrades. The model gets more "average" without obviously getting worse; until it does.</em></p>

        <!-- Subsection: Data provenance, watermarking, pre-2022 text value -->
        <h3>The Certification Problem</h3>

        <p>Data provenance becomes critical. Human-generated text, especially text predating 2022, becomes a finite resource with increasing value. Watermarking synthetic outputs is one proposed solution; certifying human authorship is another. Neither is deployed at scale.</p>

        <p><strong>The irony:</strong> the models that scraped the open web into existence may have foreclosed that strategy for their successors.</p>

        <!-- Subsection: Four scenarios - plateau, breakthrough, bifurcation, collapse -->
        <h3>Possible Futures</h3>

        <ol>
            <li><strong>Plateau:</strong> Scaling laws hit diminishing returns. Models get marginally better each year. AI becomes mature technology, like databases.</li>
            <li><strong>Breakthrough:</strong> New architectures, new training paradigms, or new data sources (robotics, video, simulation) extend the curve. The "bitter lesson" continues.</li>
            <li><strong>Bifurcation:</strong> A few players with proprietary data (Google, Meta) pull ahead. Open models stagnate. The commons erodes.</li>
            <li><strong>Collapse:</strong> Contamination degrades the entire ecosystem. Models trained in 2028 underperform models trained in 2024 on tasks requiring genuine diversity.</li>
        </ol>

        <p><em>Shannon started with a book and a finger. Eighty years later, we've consumed most of the books. What comes next depends on whether we can find something genuinely new to say.</em></p>

        <figure id="img-data-wall" class="breakout">
            <img src="images/no-more-books-01.png" alt="Empty library with scattered books" class="lightbox-trigger" data-caption="Now what?">
            <figcaption>Now what?</figcaption>
        </figure>

        <div class="source">
            <p><strong>Sources:</strong></p>
            <ol>
                <li>Villalobos, P. et al. "Will we run out of data? Limits of LLM scaling based on human-generated data." <em>Proceedings of the 41st International Conference on Machine Learning</em>, PMLR 235:49523-49544, 2024.</li>
                <li>Shumailov, I. et al. "AI models collapse when trained on recursively generated data." <em>Nature</em> 631, 755-759 (2024). <a href="https://doi.org/10.1038/s41586-024-07566-y">https://doi.org/10.1038/s41586-024-07566-y</a></li>
                <li>Villalobos, Pablo, et al. "Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning." <em>arXiv</em>, 2022, <a href="https://arxiv.org/abs/2211.04325">arxiv.org/abs/2211.04325</a>.</li>
            </ol>
        </div>

    </article>

    <!-- Lightbox Modal -->
    <div class="modal-overlay" id="imageModal">
        <span class="modal-close" id="modalClose">&times;</span>
        <img class="modal-content" id="modalImage" src="" alt="">
        <p class="modal-caption" id="modalCaption"></p>
    </div>

    <!-- BYTE Magazine Gallery Modal -->
    <div class="gallery-modal" id="byteGallery">
        <span class="gallery-close" id="galleryClose">&times;</span>
        <div class="gallery-header">
            <h3>A Travesty Generator for Micros</h3>
            <p>BYTE Magazine, November 1984 - Hugh Kenner &amp; Joseph O'Rourke</p>
        </div>
        <div class="gallery-grid">
            <div class="gallery-page"><img src="images/byte-page-01.png" alt="Page 1"><p class="gallery-page-num">Page 1 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-02.png" alt="Page 2"><p class="gallery-page-num">Page 2 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-03.png" alt="Page 3"><p class="gallery-page-num">Page 3 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-04.png" alt="Page 4"><p class="gallery-page-num">Page 4 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-05.png" alt="Page 5"><p class="gallery-page-num">Page 5 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-06.png" alt="Page 6"><p class="gallery-page-num">Page 6 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-07.png" alt="Page 7"><p class="gallery-page-num">Page 7 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-08.png" alt="Page 8"><p class="gallery-page-num">Page 8 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-09.png" alt="Page 9"><p class="gallery-page-num">Page 9 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-10.png" alt="Page 10"><p class="gallery-page-num">Page 10 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-11.png" alt="Page 11"><p class="gallery-page-num">Page 11 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-12.png" alt="Page 12"><p class="gallery-page-num">Page 12 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-13.png" alt="Page 13"><p class="gallery-page-num">Page 13 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-14.png" alt="Page 14"><p class="gallery-page-num">Page 14 of 15</p></div>
            <div class="gallery-page"><img src="images/byte-page-15.png" alt="Page 15"><p class="gallery-page-num">Page 15 of 15</p></div>
        </div>
    </div>

    <script>
        (function() {
            // Lightbox Modal
            const modal = document.getElementById('imageModal');
            const modalImg = document.getElementById('modalImage');
            const modalCaption = document.getElementById('modalCaption');
            const closeBtn = document.getElementById('modalClose');

            document.querySelectorAll('.lightbox-trigger').forEach(function(img) {
                img.addEventListener('click', function() {
                    modal.classList.add('active');
                    modalImg.src = this.src;
                    modalCaption.textContent = this.dataset.caption || '';
                    document.body.style.overflow = 'hidden';
                });
            });

            function closeModal() {
                modal.classList.remove('active');
                document.body.style.overflow = '';
            }

            closeBtn.addEventListener('click', closeModal);
            modal.addEventListener('click', function(e) {
                if (e.target === modal) closeModal();
            });

            // BYTE Magazine Gallery
            const gallery = document.getElementById('byteGallery');
            const galleryTrigger = document.getElementById('byteGalleryTrigger');
            const galleryClose = document.getElementById('galleryClose');

            galleryTrigger.addEventListener('click', function() {
                gallery.classList.add('active');
                document.body.style.overflow = 'hidden';
            });

            function closeGallery() {
                gallery.classList.remove('active');
                document.body.style.overflow = '';
            }

            galleryClose.addEventListener('click', closeGallery);

            // Close either modal with Escape
            document.addEventListener('keydown', function(e) {
                if (e.key === 'Escape') {
                    closeModal();
                    closeGallery();
                }
            });
        })();
    </script>
</body>
</html>
