<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ğŸ’¬</text></svg>">
    <title>Why Non-English Speakers Pay More for AI</title>
    <style>
        :root {
            --text-color: #292929;
            --bg-color: #fff;
            --accent-color: #1a8917;
            --border-color: #e6e6e6;
            --code-bg: #f4f4f4;
        }

        * {
            box-sizing: border-box;
        }

        body {
            font-family: charter, Georgia, Cambria, "Times New Roman", Times, serif;
            font-size: 21px;
            line-height: 1.58;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 0;
        }

        article {
            max-width: 680px;
            margin: 0 auto;
            padding: 40px 24px 80px;
        }

        h1 {
            font-family: sohne, "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-size: 42px;
            font-weight: 700;
            line-height: 1.2;
            margin-bottom: 16px;
            letter-spacing: -0.011em;
        }

        h2 {
            font-family: sohne, "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-size: 28px;
            font-weight: 700;
            line-height: 1.3;
            margin-top: 56px;
            margin-bottom: 16px;
            letter-spacing: -0.009em;
        }

        h3 {
            font-family: sohne, "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-size: 22px;
            font-weight: 700;
            line-height: 1.4;
            margin-top: 40px;
            margin-bottom: 12px;
        }

        p {
            margin-bottom: 24px;
        }

        .lead {
            font-size: 24px;
            line-height: 1.5;
            color: #555;
            margin-bottom: 32px;
        }

        ul, ol {
            margin-bottom: 24px;
            padding-left: 30px;
        }

        li {
            margin-bottom: 12px;
        }

        code {
            font-family: Menlo, Monaco, "Courier New", Courier, monospace;
            font-size: 16px;
            background-color: var(--code-bg);
            padding: 2px 6px;
            border-radius: 3px;
        }

        pre {
            font-family: Menlo, Monaco, "Courier New", Courier, monospace;
            font-size: 14px;
            background-color: var(--code-bg);
            padding: 20px;
            border-radius: 4px;
            overflow-x: auto;
            margin-bottom: 24px;
            line-height: 1.5;
        }

        blockquote {
            border-left: 3px solid var(--text-color);
            margin: 32px 0;
            padding-left: 20px;
            font-style: italic;
        }

        figure {
            margin: 40px 0;
            text-align: center;
        }

        figure img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
        }

        figcaption {
            font-size: 16px;
            color: #757575;
            margin-top: 12px;
            font-style: italic;
        }

        a {
            color: inherit;
            text-decoration: underline;
        }

        a:hover {
            color: var(--accent-color);
        }

        strong {
            font-weight: 700;
        }

        .back-link {
            display: inline-block;
            margin-bottom: 24px;
            font-size: 16px;
            color: #757575;
        }

        hr {
            border: none;
            border-top: 1px solid var(--border-color);
            margin: 48px 0;
        }

        .separator {
            text-align: center;
            margin: 48px 0;
            color: #757575;
        }

        .references {
            font-size: 16px;
            line-height: 1.6;
        }

        .references p {
            margin-bottom: 12px;
        }

        /* Data table styling */
        table {
            width: 100%;
            border-collapse: collapse;
            font-family: Menlo, Monaco, "Courier New", Courier, monospace;
            font-size: 14px;
            margin: 24px 0;
        }

        th, td {
            text-align: left;
            padding: 8px 12px;
            border-bottom: 1px solid var(--border-color);
        }

        th {
            font-weight: 600;
            color: #555;
        }

        tr:hover {
            background-color: #fafafa;
        }

        .ratio-low { color: #22c55e; }
        .ratio-mid { color: #eab308; }
        .ratio-high { color: #f97316; }
        .ratio-extreme { color: #ef4444; }

        /* Bar chart styling */
        .bar-chart {
            background-color: var(--code-bg);
            padding: 20px;
            border-radius: 4px;
            margin: 24px 0;
            font-family: Menlo, Monaco, "Courier New", Courier, monospace;
            font-size: 13px;
        }

        .bar-row {
            display: flex;
            align-items: center;
            margin-bottom: 8px;
        }

        .bar-label {
            width: 90px;
            flex-shrink: 0;
        }

        .bar-container {
            flex-grow: 1;
            height: 20px;
            background-color: #e5e5e5;
            border-radius: 2px;
            overflow: hidden;
        }

        .bar-fill {
            height: 100%;
            background-color: #292929;
        }

        .bar-value {
            width: 50px;
            text-align: right;
            margin-left: 8px;
            flex-shrink: 0;
        }

        .chart-legend {
            margin-top: 16px;
            font-size: 12px;
            color: #757575;
        }

        /* Interactive demo embed */
        .interactive-demo {
            margin: 40px 0;
        }

        .interactive-demo iframe {
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }

        @media (max-width: 728px) {
            body {
                font-size: 18px;
            }

            h1 {
                font-size: 32px;
            }

            h2 {
                font-size: 24px;
            }

            article {
                padding: 24px 16px 60px;
            }

            table {
                font-size: 12px;
            }

            th, td {
                padding: 6px 8px;
            }

            pre {
                font-size: 12px;
                padding: 16px;
            }
        }
    </style>
</head>
<body>
    <article>
        <a href="../" class="back-link">&larr; All Articles</a>

        <h1>Why Non-English Speakers Pay More for AI</h1>

        <p class="lead">Same meaning, different price. The hidden cost of tokenization.</p>

        <p>Type "I, for one, welcome our new insect overlords" into GPT-4.</p>

        <p>12 (sub-word) tokens.</p>

        <p>Type it in Japanese. Twenty-two tokens.<br>
        Type it in Hindi. Fifty-six tokens.<br>
        Type it in Tamil. Eighty-six tokens.</p>

        <p>Same groveling to our arthropod masters. Same model. Up to 7x the cost.</p>

        <p>At first glance, this looks unfair. Same intent, same semantics, wildly different bills. But the tokenizer isn't measuring meaning. It's measuring how well your language was economically represented at training time.</p>

        <p>Kent Brockman surrenders in 12 tokens. Tamil speakers need 86.</p>

        <figure id="img-overlords">
            <img id="overlord-img" alt="People kneeling before insect overlord">
            <figcaption>The cost of surrender depends on your alphabet.</figcaption>
        </figure>
        <script>
            const overlordImages = ['../images/multilingual-tax/praying-mantis-overlords.png', '../images/multilingual-tax/news-anchor-insect-overlords.png'];
            document.getElementById('overlord-img').src = overlordImages[Math.floor(Math.random() * overlordImages.length)];
        </script>

        <p>At first glance, this looks unfair. Same intent, same semantics, same model, wildly different token counts. But the tokenizer isn't measuring meaning. It's measuring <em>how well your language was economically represented at training time</em>.</p>

        <table>
            <thead>
                <tr>
                    <th>Language</th>
                    <th>Tokens</th>
                    <th>Ratio</th>
                    <th>Text</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>English</td><td>12</td><td class="ratio-low">1.0x</td><td>I, for one, welcome our new insect overlords.</td></tr>
                <tr><td>German</td><td>17</td><td class="ratio-low">1.4x</td><td>Ich fÃ¼r meinen Teil begrÃ¼ÃŸe unsere neuen Insekten-Oberherren.</td></tr>
                <tr><td>Dutch</td><td>17</td><td class="ratio-low">1.4x</td><td>Ik, voor mijn part, verwelkom onze nieuwe insectenheersers.</td></tr>
                <tr><td>Indonesian</td><td>18</td><td class="ratio-low">1.5x</td><td>Saya, untuk satu, menyambut penguasa serangga baru kita.</td></tr>
                <tr><td>French</td><td>19</td><td class="ratio-mid">1.6x</td><td>Pour ma part, je souhaite la bienvenue Ã  nos nouveaux maÃ®tres insectes.</td></tr>
                <tr><td>Spanish</td><td>19</td><td class="ratio-mid">1.6x</td><td>Yo, por mi parte, doy la bienvenida a nuestros nuevos seÃ±ores insectos.</td></tr>
                <tr><td>Czech</td><td>19</td><td class="ratio-mid">1.6x</td><td>JÃ¡ osobnÄ› vÃ­tÃ¡m naÅ¡e novÃ© hmyzÃ­ pÃ¡ny.</td></tr>
                <tr><td>Norwegian</td><td>20</td><td class="ratio-mid">1.7x</td><td>Jeg, for min del, Ã¸nsker vÃ¥re nye insektherskere velkommen.</td></tr>
                <tr><td>Swedish</td><td>20</td><td class="ratio-mid">1.7x</td><td>Jag, fÃ¶r min del, vÃ¤lkomnar vÃ¥ra nya insektshÃ¤rskare.</td></tr>
                <tr><td>Italian</td><td>20</td><td class="ratio-mid">1.7x</td><td>Io, per primo, do il benvenuto ai nostri nuovi signori insetti.</td></tr>
                <tr><td>Danish</td><td>20</td><td class="ratio-mid">1.7x</td><td>Jeg, for min del, byder vores nye insektherskere velkommen.</td></tr>
                <tr><td>Polish</td><td>21</td><td class="ratio-mid">1.8x</td><td>Ja, ze swojej strony, witam naszych nowych wÅ‚adcÃ³w owadÃ³w.</td></tr>
                <tr><td>Turkish</td><td>22</td><td class="ratio-mid">1.8x</td><td>Ben, kendi adÄ±ma, yeni bÃ¶cek efendilerimizi karÅŸÄ±lÄ±yorum.</td></tr>
                <tr><td>Japanese</td><td>22</td><td class="ratio-mid">1.8x</td><td>ç§ã¯æ–°ã—ã„æ˜†è™«ã®æ”¯é…è€…ãŸã¡ã‚’æ­“è¿ã—ã¾ã™ã€‚</td></tr>
                <tr><td>Portuguese</td><td>23</td><td class="ratio-mid">1.9x</td><td>Eu, por minha parte, dou as boas-vindas aos nossos novos senhores insetos.</td></tr>
                <tr><td>Hungarian</td><td>23</td><td class="ratio-mid">1.9x</td><td>Ã‰n a magam rÃ©szÃ©rÅ‘l Ã¼dvÃ¶zlÃ¶m Ãºj rovar urainkat.</td></tr>
                <tr><td>Chinese</td><td>23</td><td class="ratio-mid">1.9x</td><td>æˆ‘ï¼Œä½œä¸ºå…¶ä¸­ä¸€å‘˜ï¼Œæ¬¢è¿æˆ‘ä»¬æ–°çš„æ˜†è™«éœ¸ä¸»ã€‚</td></tr>
                <tr><td>Romanian</td><td>24</td><td class="ratio-mid">2.0x</td><td>Eu, personal, Ã®i Ã®ntÃ¢mpin pe noii noÈ™tri stÄƒpÃ¢ni insecte.</td></tr>
                <tr><td>Finnish</td><td>25</td><td class="ratio-high">2.1x</td><td>MinÃ¤ puolestani toivotan uudet hyÃ¶nteisherramme tervetulleiksi.</td></tr>
                <tr><td>Russian</td><td>28</td><td class="ratio-high">2.3x</td><td>Ğ¯, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑ Ğ½Ğ°ÑˆĞ¸Ñ… Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµĞ»Ğ¸Ñ‚ĞµĞ»ĞµĞ¹-Ğ½Ğ°ÑĞµĞºĞ¾Ğ¼Ñ‹Ñ….</td></tr>
                <tr><td>Ukrainian</td><td>28</td><td class="ratio-high">2.3x</td><td>Ğ¯, Ğ½Ğ°Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´, Ğ²Ñ–Ñ‚Ğ°Ñ Ğ½Ğ°ÑˆĞ¸Ñ… Ğ½Ğ¾Ğ²Ğ¸Ñ… Ğ¿Ğ¾Ğ²ĞµĞ»Ğ¸Ñ‚ĞµĞ»Ñ–Ğ²-ĞºĞ¾Ğ¼Ğ°Ñ….</td></tr>
                <tr><td>Swahili</td><td>29</td><td class="ratio-high">2.4x</td><td>Mimi, kwa upande wangu, nawakaribisha watawala wetu wapya wa wadudu.</td></tr>
                <tr><td>Arabic</td><td>30</td><td class="ratio-high">2.5x</td><td>Ø£Ù†Ø§ØŒ Ù…Ù† Ø¬Ù‡ØªÙŠØŒ Ø£Ø±Ø­Ø¨ Ø¨Ø³Ø§Ø¯ØªÙ†Ø§ Ø§Ù„Ø­Ø´Ø±Ø§Øª Ø§Ù„Ø¬Ø¯Ø¯.</td></tr>
                <tr><td>Korean</td><td>32</td><td class="ratio-high">2.7x</td><td>ì €ëŠ” ê°œì¸ì ìœ¼ë¡œ ìš°ë¦¬ì˜ ìƒˆë¡œìš´ ê³¤ì¶© ì§€ë°°ìë“¤ì„ í™˜ì˜í•©ë‹ˆë‹¤.</td></tr>
                <tr><td>Vietnamese</td><td>34</td><td class="ratio-high">2.8x</td><td>TÃ´i, vá» pháº§n mÃ¬nh, chÃ o Ä‘Ã³n nhá»¯ng chÃºa tá»ƒ cÃ´n trÃ¹ng má»›i cá»§a chÃºng ta.</td></tr>
                <tr><td>Thai</td><td>44</td><td class="ratio-extreme">3.7x</td><td>à¸œà¸¡ à¹ƒà¸™à¸ªà¹ˆà¸§à¸™à¸•à¸±à¸§ à¸¢à¸´à¸™à¸”à¸µà¸•à¹‰à¸­à¸™à¸£à¸±à¸šà¹€à¸ˆà¹‰à¸²à¸™à¸²à¸¢à¹à¸¡à¸¥à¸‡à¸„à¸™à¹ƒà¸«à¸¡à¹ˆà¸‚à¸­à¸‡à¹€à¸£à¸²</td></tr>
                <tr><td>Hebrew</td><td>46</td><td class="ratio-extreme">3.8x</td><td>×× ×™, ××¦×“×™, ××§×‘×œ ×‘×‘×¨×›×” ××ª ××“×•× ×™ ×”×—×¨×§×™× ×”×—×“×©×™× ×©×œ× ×•.</td></tr>
                <tr><td>Greek</td><td>54</td><td class="ratio-extreme">4.5x</td><td>Î•Î³Ï, Ï€ÏÎ¿ÏƒÏ‰Ï€Î¹ÎºÎ¬, ÎºÎ±Î»Ï‰ÏƒÎ¿ÏÎ¯Î¶Ï‰ Ï„Î¿Ï…Ï‚ Î½Î­Î¿Ï…Ï‚ Î¼Î±Ï‚ ÎºÏ…ÏÎ¯Î±ÏÏ‡Î¿Ï…Ï‚ Î­Î½Ï„Î¿Î¼Î±.</td></tr>
                <tr><td>Hindi</td><td>56</td><td class="ratio-extreme">4.7x</td><td>à¤®à¥ˆà¤‚, à¤…à¤ªà¤¨à¥€ à¤“à¤° à¤¸à¥‡, à¤¹à¤®à¤¾à¤°à¥‡ à¤¨à¤ à¤•à¥€à¤Ÿ à¤¸à¥à¤µà¤¾à¤®à¤¿à¤¯à¥‹à¤‚ à¤•à¤¾ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥‚à¤‚à¥¤</td></tr>
                <tr><td>Bengali</td><td>75</td><td class="ratio-extreme">6.2x</td><td>à¦†à¦®à¦¿, à¦†à¦®à¦¾à¦° à¦ªà¦•à§à¦· à¦¥à§‡à¦•à§‡, à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¨à¦¤à§à¦¨ à¦ªà§‹à¦•à¦¾ à¦ªà§à¦°à¦­à§à¦¦à§‡à¦° à¦¸à§à¦¬à¦¾à¦—à¦¤ à¦œà¦¾à¦¨à¦¾à¦‡à¥¤</td></tr>
                <tr><td>Tamil</td><td>86</td><td class="ratio-extreme">7.2x</td><td>à®¨à®¾à®©à¯, à®à®©à®¤à¯ à®ªà®™à¯à®•à®¿à®±à¯à®•à¯, à®à®™à¯à®•à®³à¯ à®ªà¯à®¤à®¿à®¯ à®ªà¯‚à®šà¯à®šà®¿ à®…à®¤à®¿à®ªà®¤à®¿à®•à®³à¯ˆ à®µà®°à®µà¯‡à®±à¯à®•à®¿à®±à¯‡à®©à¯.</td></tr>
            </tbody>
        </table>

        <p>That outcome follows directly from history.</p>

        <p><a href="../bpe-history/">Byte Pair Encoding began as a compression algorithm in the 1990s</a>. Engineers designed it to shrink arbitrary data by repeatedly merging the most frequent byte sequences. It did not emerge from linguistics. It emerged from pragmatism. When modern language models adopted BPE, they inherited that logic intact.</p>

        <figure class="interactive-demo">
            <iframe
                src="https://codesandbox.io/embed/cw89l8?fontsize=14&hidenavigation=1&theme=dark&view=preview"
                style="width: 100%; height: 500px; border: 0; border-radius: 8px; overflow: hidden;"
                title="Multilingual Token Calculator"
                allow="accelerometer; ambient-light-sensor; camera; encrypted-media; geolocation; gyroscope; hid; microphone; midi; payment; usb; vr; xr-spatial-tracking"
                sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts">
            </iframe>
            <figcaption>Try this out as an interactive demo</figcaption>
        </figure>

        <p>Frequency determines value.</p>

        <p>Substrings that appear often in the training data become single, cheap tokens. Substrings that appear less often remain fragmented into smaller pieces. English benefits because English dominates the corpus. Japanese fragments because the tokenizer encountered its patterns less often during training.</p>

        <p>The tokenizer does not struggle with Japanese. It simply stores Japanese inefficiently.</p>

        <p>That inefficiency shows up as cost.</p>

        <p>Type "Hello, how are you?" in Tamil. Twenty-three tokens.</p>

        <p>Languages the tokenizer saw less frequently get understood less efficiently. And the difference isn't small.</p>

        <p>Tamil speakers pay 4x what English speakers pay for the same information.</p>

        <p>Token counts drive pricing, context limits, and truncation behavior. When Japanese text expands into more tokens, fewer ideas fit into the same context window. Long prompts collapse sooner. Retrieval pipelines return less semantic content per request. None of this looks dramatic in isolation. Together, it reshapes what multilingual users can afford to do.</p>

        <p>This behavior does not reflect a mistake. It reflects a design tradeoff. Engineers optimized tokenization for scale, speed, and statistical coverage, not for linguistic equity. A compression algorithm rewards what it sees most often. It always has.</p>

        <p>In English corpora, a very small set of function words accounts for an outsized share of all tokens:</p>

        <ul>
            <li><strong>Articles:</strong> the, a</li>
            <li><strong>Prepositions:</strong> of, to, in, for</li>
            <li><strong>Pronouns:</strong> I, you, we, it</li>
            <li><strong>Auxiliaries:</strong> is, are, was, have</li>
            <li><strong>Conjunctions:</strong> and, but, or</li>
            <li><strong>Particles and markers:</strong> not, that</li>
        </ul>

        <p>Depending on corpus and counting method, the top 50 to 100 words in English often cover 45-60% of all word occurrences in running text. Most of those words are function words.</p>

        <table>
            <thead>
                <tr>
                    <th>Language</th>
                    <th>Type</th>
                    <th>Content Words (Lexicon)</th>
                    <th>Function Words (Lexicon)</th>
                    <th>Content Words (Usage)</th>
                    <th>Function Words (Usage)</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>English</td><td>Analytic</td><td>~99.7% (~170K words)</td><td>~0.3% (~300 words)</td><td>~45%</td><td>~55%</td></tr>
                <tr><td>Chinese</td><td>Isolating</td><td>~99.5% (~50K words)</td><td>~0.5% (~250 words)</td><td>~55%</td><td>~45%</td></tr>
                <tr><td>Japanese</td><td>Agglutinative</td><td>~99.8% (~50K words)</td><td>~0.2% (~100 words)</td><td>~60%</td><td>~40%</td></tr>
                <tr><td>Russian</td><td>Fusional</td><td>~99.5% (~150K words)</td><td>~0.5% (~400 words)</td><td>~65%</td><td>~35%</td></tr>
                <tr><td>Arabic</td><td>Fusional</td><td>~99.6% (~60K words)</td><td>~0.4% (~200 words)</td><td>~70%</td><td>~30%</td></tr>
                <tr><td>Korean</td><td>Agglutinative</td><td>~99.8% (~50K words)</td><td>~0.2% (~80 words)</td><td>~75%</td><td>~25%</td></tr>
                <tr><td>Turkish</td><td>Agglutinative</td><td>~99.9% (~100K words)</td><td>~0.1% (~50 words)</td><td>~80%</td><td>~20%</td></tr>
                <tr><td>Hungarian</td><td>Agglutinative</td><td>~99.9% (~80K words)</td><td>~0.1% (~40 words)</td><td>~82%</td><td>~18%</td></tr>
                <tr><td>Finnish</td><td>Agglutinative</td><td>~99.9% (~90K words)</td><td>~0.1% (~40 words)</td><td>~85%</td><td>~15%</td></tr>
                <tr><td>Swahili</td><td>Agglutinative</td><td>~99.9% (~50K words)</td><td>~0.1% (~30 words)</td><td>~88%</td><td>~12%</td></tr>
                <tr><td>Inuktitut</td><td>Polysynthetic</td><td>~99.95% (~10K roots)</td><td>~0.05% (~20 words)</td><td>~95%</td><td>~5%</td></tr>
            </tbody>
        </table>

        <p>English concentrates usage into a few function words.</p>

        <div class="bar-chart">
            <p style="margin-bottom: 16px; font-weight: 600;">Function Words in Actual Usage (sorted high &rarr; low)</p>
            <div class="bar-row"><span class="bar-label">English</span><div class="bar-container"><div class="bar-fill" style="width: 55%;"></div></div><span class="bar-value">55%</span></div>
            <div class="bar-row"><span class="bar-label">Chinese</span><div class="bar-container"><div class="bar-fill" style="width: 45%;"></div></div><span class="bar-value">45%</span></div>
            <div class="bar-row"><span class="bar-label">Japanese</span><div class="bar-container"><div class="bar-fill" style="width: 40%;"></div></div><span class="bar-value">40%</span></div>
            <div class="bar-row"><span class="bar-label">Russian</span><div class="bar-container"><div class="bar-fill" style="width: 35%;"></div></div><span class="bar-value">35%</span></div>
            <div class="bar-row"><span class="bar-label">Arabic</span><div class="bar-container"><div class="bar-fill" style="width: 30%;"></div></div><span class="bar-value">30%</span></div>
            <div class="bar-row"><span class="bar-label">Korean</span><div class="bar-container"><div class="bar-fill" style="width: 25%;"></div></div><span class="bar-value">25%</span></div>
            <div class="bar-row"><span class="bar-label">Turkish</span><div class="bar-container"><div class="bar-fill" style="width: 20%;"></div></div><span class="bar-value">20%</span></div>
            <div class="bar-row"><span class="bar-label">Hungarian</span><div class="bar-container"><div class="bar-fill" style="width: 18%;"></div></div><span class="bar-value">18%</span></div>
            <div class="bar-row"><span class="bar-label">Finnish</span><div class="bar-container"><div class="bar-fill" style="width: 15%;"></div></div><span class="bar-value">15%</span></div>
            <div class="bar-row"><span class="bar-label">Swahili</span><div class="bar-container"><div class="bar-fill" style="width: 12%;"></div></div><span class="bar-value">12%</span></div>
            <div class="bar-row"><span class="bar-label">Inuktitut</span><div class="bar-container"><div class="bar-fill" style="width: 5%;"></div></div><span class="bar-value">5%</span></div>
            <div class="chart-legend">&#9608; Function words &nbsp;&nbsp; &#9617; Content words</div>
        </div>

        <p>BPE does not care what compresses.</p>

        <p>It only cares what repeats.</p>

        <p>English repeats for two reasons that compound:</p>

        <ol>
            <li><strong>Corpus dominance</strong><br>English appears far more often than any other language in LLM training data. That guarantees its surface forms get seen orders of magnitude more times.</li>
            <li><strong>Internal repetition structure</strong><br>Within English, a small set of function words accounts for roughly half of all usage. Those words repeat constantly and with minimal variation.</li>
        </ol>

        <p>Corpus dominance determines <em>which</em> language benefits.</p>

        <p>Function-word concentration determines <em>how much</em> it benefits.</p>

        <p>Neither alone is sufficient.</p>

        <p>Linguists classify languages by how they package meaning into words. Some languages use many small, separate words. Others pack entire sentences into single, complex words. This structural difference has profound implications for how well BPE tokenization can compress text.</p>

        <p><strong>The four major types:</strong></p>

        <ul>
            <li><strong>Analytic:</strong> Grammar lives in separate function words: "the," "will," "of." Words stay short and stable. (English, Chinese, Vietnamese)</li>
            <li><strong>Agglutinative:</strong> Grammar attaches as chains of suffixes. One word can encode subject, tense, mood, and more. (Turkish, Finnish, Korean, Swahili)</li>
            <li><strong>Fusional:</strong> Single affixes encode multiple grammatical features at once, often irregularly. (Russian, Arabic, Spanish)</li>
            <li><strong>Polysynthetic:</strong> Entire sentences compress into single words. Extreme morphological productivity. (Inuktitut, Mohawk, Yupik)</li>
        </ul>

        <p><strong>Why this matters for tokenization:</strong></p>

        <p>BPE learns to compress text by finding repeated byte sequences. Languages with many short, stable, frequently-repeated words give BPE lots of reusable patterns. Languages that encode meaning in long, productive word forms produce fewer exact repetitions and hit a compression ceiling that no amount of training data can overcome.</p>

        <p>The charts below show five properties that BPE exploits for compression. Notice how the polygon shrinks from Analytic &rarr; Polysynthetic. The smaller the polygon, the harder it is for BPE to achieve efficient tokenization regardless of how much training data exists.</p>

        <figure class="interactive-demo">
            <iframe
                src="https://codesandbox.io/embed/fghj4p?fontsize=14&hidenavigation=1&theme=dark&view=preview"
                style="width: 100%; height: 600px; border: 0; border-radius: 8px; overflow: hidden;"
                title="Morphological Types Radar Charts"
                allow="accelerometer; ambient-light-sensor; camera; encrypted-media; geolocation; gyroscope; hid; microphone; midi; payment; usb; vr; xr-spatial-tracking"
                sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts">
            </iframe>
            <figcaption>Corpus dominance lowers cost; morphology sets the floor.</figcaption>
        </figure>

        <p>Token cost is not a property of meaning. It emerges from repetition, exposure, and structure. Corpus dominance decides which language benefits. Morphology decides how far that benefit can go.</p>

        <h3>Try It Yourself: The Compression Ceiling Explorer</h3>

        <p>The charts above show static snapshots, but the real insight comes from seeing how these properties change (or don't) as training data grows.</p>

        <p>The interactive demo below lets you experiment with two variables:</p>

        <ol>
            <li><strong>Language type:</strong> Select Analytic (English), Agglutinative (Turkish), Fusional (Russian), or Polysynthetic (Inuktitut)</li>
            <li><strong>Corpus dominance:</strong> Slide from 1% to 100% to simulate what happens as a language gets more representation in training data</li>
        </ol>

        <p>Watch what happens as you drag the slider. For English, the polygon expands dramatically as more data means more compression. But for Polysynthetic languages, something different happens: the polygon grows, but hits a wall. Even at 100% corpus dominance, it can't reach the compression levels that English achieves at 50%.</p>

        <p>This is the ceiling in action. Three of the five axes (Word Boundary Clarity, Function Word Frequency, Surface Form Stability) are <em>fixed</em> because they're determined by grammar, not data. Only two axes (Byte Reuse Potential, Compression Achieved) respond to more training data.</p>

        <p>Can you try make Polysynthetic beat Analytic?</p>

        <p>Not likely. English not only has corpus dominance, it has the highest structural ceiling. Even in a hypothetical world where Inuktitut dominated the training corpus, it would still tokenize less efficiently than English does today.</p>

        <figure class="interactive-demo">
            <iframe
                src="https://codesandbox.io/embed/z43ltr?fontsize=14&hidenavigation=1&theme=dark&view=preview"
                style="width: 100%; height: 700px; border: 0; border-radius: 8px; overflow: hidden;"
                title="Compression Ceiling Explorer"
                allow="accelerometer; ambient-light-sensor; camera; encrypted-media; geolocation; gyroscope; hid; microphone; midi; payment; usb; vr; xr-spatial-tracking"
                sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts">
            </iframe>
            <figcaption>It's not just about how much data exists. It's about what compression is possible.</figcaption>
        </figure>

        <p>English wins twice: once from corpus dominance (it has the most training data), and again from structural advantage (its grammar creates the most compressible patterns). Other languages can close the first gap with more data. They cannot close the second.</p>

        <p>The ceiling is baked into the grammar itself.</p>

        <h2>The Compounding Costs</h2>

        <h3>Direct Financial Cost</h3>

        <p>At GPT-4 pricing ($2.50 per million input tokens):</p>

        <table>
            <thead>
                <tr>
                    <th>Language</th>
                    <th>Tokens/1000 words</th>
                    <th>Cost</th>
                    <th>Ratio</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>English</td><td>~1,300</td><td>$0.00325</td><td class="ratio-low">1.0x</td></tr>
                <tr><td>Spanish</td><td>~1,800</td><td>$0.00450</td><td class="ratio-low">1.4x</td></tr>
                <tr><td>Russian</td><td>~3,000</td><td>$0.00750</td><td class="ratio-high">2.3x</td></tr>
                <tr><td>Arabic</td><td>~3,250</td><td>$0.00813</td><td class="ratio-high">2.5x</td></tr>
                <tr><td>Tamil</td><td>~9,400</td><td>$0.02350</td><td class="ratio-extreme">7.2x</td></tr>
            </tbody>
        </table>

        <p>For a company processing millions of customer queries in Tamil, the infrastructure cost is <strong>7x higher</strong> than an English-only competitor.</p>

        <h3>Context Window Tax</h3>

        <p>GPT-4's 128K context window sounds huge. But in tokens, not characters.</p>

        <p>If you're working in Tamil, that 128K window holds roughly <strong>14K words</strong> of content.</p>

        <figure id="img-globe">
            <img src="images/globe-puzzle-1.png" alt="Globe made of puzzle pieces with one hemisphere fragmented">
            <figcaption>The pieces are all there. Some just cost more.</figcaption>
        </figure>

        <p>In English, the same window holds <strong>~100K words</strong>.</p>

        <p>Same advertised limit. 7x less actual capacity.</p>

        <h3>Quality Degradation</h3>

        <p>Fragmented tokens don't just cost more. They may perform worse.</p>

        <p>When a word is split into pieces, the model must:</p>

        <ol>
            <li>Recognize the pieces as belonging together</li>
            <li>Compose meaning across token boundaries</li>
            <li>Handle the increased sequence length</li>
        </ol>

        <p><a href="https://aclanthology.org/2021.findings-acl.110/">Research suggests models perform better on languages with more efficient tokenization</a>. The tokenizer isn't neutral infrastructure. It's a performance bottleneck.</p>

        <h2>Who Pays Most?</h2>

        <p><strong>Winners:</strong></p>
        <ul>
            <li>English, German, French, Spanish</li>
            <li>Chinese (dedicated vocabulary space)</li>
            <li>Code (heavily represented in training)</li>
        </ul>

        <p><strong>Moderate tax (2-3x):</strong></p>
        <ul>
            <li>Russian, Japanese, Korean</li>
            <li>Portuguese, Italian, Dutch</li>
        </ul>

        <p><strong>Heavy tax (3-5x+):</strong></p>
        <ul>
            <li>Arabic, Hebrew, Persian</li>
            <li>Hindi, Tamil, Bengali, Telugu</li>
            <li>Thai, Vietnamese, Indonesian</li>
            <li>Swahili, Yoruba, Amharic</li>
            <li>Most languages spoken by &lt;100M people</li>
        </ul>

        <p>GPT-4o doubled vocabulary size (~200K tokens vs ~100K). This helps somewhat, allocating more tokens to non-English text.</p>

        <p>Some models train tokenizers on balanced multilingual corpora:</p>

        <ul>
            <li><strong>BLOOM:</strong> Explicitly balanced across 46 languages</li>
            <li><strong>mT5:</strong> Trained on mC4, covering 101 languages</li>
            <li><strong>XLM-RoBERTa:</strong> Designed for cross-lingual transfer</li>
        </ul>

        <p>These narrow the gap but don't eliminate it.</p>

        <h3>Language-Specific Models</h3>

        <p>For high-stakes applications, dedicated models exist:</p>

        <ul>
            <li><strong>Japanese:</strong> rinna, ELYZA</li>
            <li><strong>Chinese:</strong> ChatGLM, Qwen</li>
            <li><strong>Arabic:</strong> Jais, AraGPT2</li>
        </ul>

        <p>These optimize tokenization for their target language but sacrifice English performance.</p>

        <hr>

        <div class="references">
            <h2>References</h2>

            <p>1. Petrov, A., et al. (2023). <a href="https://arxiv.org/abs/2305.15425">"Language Model Tokenizers Introduce Unfairness Between Languages."</a> arXiv.</p>

            <p>2. Ahia, O., et al. (2023). <a href="https://aclanthology.org/2023.emnlp-main.614/">"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models."</a> EMNLP.</p>

            <p>3. Rust, P., et al. (2021). <a href="https://aclanthology.org/2021.findings-acl.110/">"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models."</a> ACL.</p>

            <p>4. OpenAI. (2023). <a href="https://github.com/openai/tiktoken">tiktoken</a>. GitHub.</p>

            <p>5. Conneau, A., et al. (2020). <a href="https://aclanthology.org/2020.acl-main.747/">"Unsupervised Cross-lingual Representation Learning at Scale."</a> ACL.</p>
        </div>

    </article>
</body>
</html>
